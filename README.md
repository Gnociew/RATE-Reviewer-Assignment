# RATE: Reviewer Profiling and Annotation-free Training for Expertise Ranking in Peer Review Systems

This repository contains the reproduction scripts and documentation for the RATE experiment.

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ README.md          # This documentation
â”œâ”€â”€ main.py                 # Main entry point for reproduction
â”œâ”€â”€ scripts/                # Core logic scripts
â”‚   â””â”€â”€ RATE.py             # Similarity calculation logic
â”œâ”€â”€ train/                  # Training scripts
â”‚   â””â”€â”€ training.py         # Original training code
â”œâ”€â”€ evaluation_script.py    # Evaluation metrics calculation
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ configs/                # Configuration files
â”‚   â””â”€â”€ example_config.yaml # Example configuration
â”œâ”€â”€ data/                   # Evaluation datasets
â”‚   â”œâ”€â”€ evaluations_pc.json # Paper-Centric evaluation data
â”‚   â”œâ”€â”€ evaluations_rc.json # Reviewer-Centric evaluation data
â”‚   â””â”€â”€ keywords.json       # Keyword mapping file
â””â”€â”€ predictions/            # Generated prediction files
```

## ğŸš€ Getting Started

### 1. Environment Setup

Ensure you have the required dependencies installed:

```bash
pip install -r requirements.txt
```

### 2. Reproduction Steps

We provide a `main.py` script that orchestrates the reproduction process. It uses `scripts/RATE.py` for inference and `evaluation_script.py` for metrics calculation.


** Option 1: Run with Config File **

We have prepared a configuration file for one-click execution.

### Model Checkpoints
We will provide two model checkpoints on Hugging Face:
- **RATE-0.6B**: [Hugging Face Link](https://huggingface.co/) (Coming Soon)
- **RATE-8B**: [Hugging Face Link](https://huggingface.co/) (Coming Soon)

```bash
python main.py --config configs/example_config.yaml
```

This will:
1.  Generate `predictions/RATE_pc.json` and `predictions/RATE_rc.json`.
2.  Automatically run evaluation and print the Accuracy and Loss metrics.

### 3. Evaluation

If you want to run evaluation separately on existing prediction files:

```bash
python evaluation_script.py --pred_paths predictions/RATE_pc.json predictions/RATE_rc.json
```
